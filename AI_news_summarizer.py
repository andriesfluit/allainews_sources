{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andriesfluit/allainews_sources/blob/main/AI_news_summarizer.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42BIUZiRnqyY",
        "outputId": "db4b78f8-75de-483b-bb7d-3b49f4f512af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.2)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.7.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install feedparser newspaper3k transformers torch schedule nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W_FSuht2ox24",
        "outputId": "1d27a667-cb87-4ec9-a034-3bb0477afa2e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing feed: https://www.theguardian.com/uk/technology/artificialintelligence/rss\n",
            "Number of entries: 0\n",
            "Processing feed: https://importai.substack.com/feed\n",
            "Number of entries: 20\n",
            "Processing article: Import AI 379: FlashAttention-3; Elon's AGI datacenter; distributed training.\n",
            "Attempting to summarize text of length: 13216\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "FlashAttention-3 makes it more efficient to train AI systems:\n",
            "\n",
            "…Significant efficiency improvements…\n",
            "\n",
            "Researchers with Colfax Research, Meta, NVIDIA, Georgia Tech, Princeton University, and Together.ai have released FlashAttention-3, the latest version of a drop-in replacement for some of the attention mechanism...\n",
            "Successfully processed: Import AI 379: FlashAttention-3; Elon's AGI datacenter; distributed training.\n",
            "Processing article: Import AI 378: AI transcendence; Tencent's one billion synthetic personas, Project Naptime\n",
            "Attempting to summarize text of length: 16834\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Google beats a hard security benchmark, showing how unoptimized today's LLMs are:\n",
            "\n",
            "…Take one LLM plus some well-designed scaffolding and a hard benchmark gets 'solved'...\n",
            "\n",
            "Google has published details on Project Naptime, a software framework Google has built to help it use LLMs for vulnerability discovery in cod...\n",
            "Successfully processed: Import AI 378: AI transcendence; Tencent's one billion synthetic personas, Project Naptime\n",
            "Processing article: Import AI 377: Voice cloning is here; MIRI's policy objective; and a new hard AGI benchmark\n",
            "Attempting to summarize text of length: 12431\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Microsoft shows that human-level voice cloning is here:\n",
            "\n",
            "…VALL-E 2 portends the wild synthetic voice future - but it's not being released (yet)...\n",
            "\n",
            "Microsoft has made further progress in text-to-speech synthesis with VALL-E 2, a system that can generate extremely good voice samples for arbitrary sentences from a...\n",
            "Successfully processed: Import AI 377: Voice cloning is here; MIRI's policy objective; and a new hard AGI benchmark\n",
            "Processing article: Import AI 376: African language test; hyper-detailed image descriptions; 1,000 hours of Meerkats.\n",
            "Attempting to summarize text of length: 12196\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "A very short issue this week as I spent the weekend solo parenting the wee beasty.\n",
            "\n",
            "Scientists release 1,000+ hours of wild meerkat audio; train model on it:\n",
            "\n",
            "…If we want to understand how animals communicate, we might as well start with meerkats…\n",
            "\n",
            "A multi-disciplinary group of researchers have built MeerKAT, a ...\n",
            "Successfully processed: Import AI 376: African language test; hyper-detailed image descriptions; 1,000 hours of Meerkats.\n",
            "Processing article: Import AI 375: GPT-2 five years later; decentralized training; new ways of thinking about consciousness and AI\n",
            "Attempting to summarize text of length: 33608\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "SPECIAL EDITION!\n",
            "\n",
            "GPT2, Five Years On:\n",
            "\n",
            "…A cold eyed reckoning about that time in 2019 when wild-eyed technologists created a (then) powerful LLM and used it to make some very confident claims about AI safety, policy, and the future of the world…\n",
            "\n",
            "Five years ago I had a few less lines in my face, a greater level...\n",
            "Successfully processed: Import AI 375: GPT-2 five years later; decentralized training; new ways of thinking about consciousness and AI\n",
            "Processing article: Import AI 374: China's military AI dataset; platonic AI; brainlike convnets\n",
            "Attempting to summarize text of length: 19062\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Berkeley researchers discover a suspiciously military-relevant Chinese dataset:\n",
            "\n",
            "…Oh you know just a normal dataset exclusively of military vessels with bounding boxes around their radar systems. Normal stuff!...\n",
            "\n",
            "UC Berkeley researchers have found a Chinese dataset named 'Zhousidun' (translation: 'Zeus's Shield...\n",
            "Successfully processed: Import AI 374: China's military AI dataset; platonic AI; brainlike convnets\n",
            "Processing article: Import AI 373: Guaranteed safety; West VS East AI attitudes; MMLU-Pro\n",
            "Attempting to summarize text of length: 13677\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "The NDIF means academia can look like the insides of the AGI shops:\n",
            "\n",
            "…APIs are all well and good, but being able to actually fiddle with weights is more valuable…\n",
            "\n",
            "Academic researchers have built the National Deep Inference Fabric (NDIF), scientific infrastructure to help them play around with large-scale, openl...\n",
            "Successfully processed: Import AI 373: Guaranteed safety; West VS East AI attitudes; MMLU-Pro\n",
            "Processing article: Import AI 372: Gibberish jailbreak; DeepSeek's great new model; Google's soccer-playing robots\n",
            "Attempting to summarize text of length: 19094\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "DeepSeek makes the best coding model in its class - and releases it as open source:\n",
            "\n",
            "…Made in China will be a thing for AI models, same as electric cars, drones, and other technologies…\n",
            "\n",
            "Chinese startup DeepSeek has built and released DeepSeek-V2, a surprisingly powerful language model. DeepSeek-V2 is a large-sc...\n",
            "Successfully processed: Import AI 372: Gibberish jailbreak; DeepSeek's great new model; Google's soccer-playing robots\n",
            "Processing article: Import AI 371: CCP vs Finetuning; why people are skeptical of AI policy; a synthesizer for a LLM\n",
            "Attempting to summarize text of length: 16158\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Why are people skeptical of AI safety policy?\n",
            "\n",
            "…A nice interview with the Alliance for the Future…\n",
            "\n",
            "Here's a good interview with Brian Chau, a founder of the DC-based AI advocacy group Alliance for the Future. Brian believes that a lot of the policy ideas being promulgated due to beliefs in AI safety are likely ...\n",
            "Successfully processed: Import AI 371: CCP vs Finetuning; why people are skeptical of AI policy; a synthesizer for a LLM\n",
            "Processing article: Import AI 370: 213 AI safety challenges; everything becomes a game; Tesla's big cluster\n",
            "Attempting to summarize text of length: 17814\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Chinese researchers build a hard benchmark for multimodal understanding:\n",
            "\n",
            "…Visual LLMs still struggle with localization and complex visual reasoning…\n",
            "\n",
            "Chinese researchers have introduced MMT-Bench, a large-scale benchmark for assessing the visual reasoning competency of language models. They test out the benchma...\n",
            "Successfully processed: Import AI 370: 213 AI safety challenges; everything becomes a game; Tesla's big cluster\n",
            "Processing article: Import AI 369: Conscious machines are possible; AI agents; the varied uses of synthetic data\n",
            "Attempting to summarize text of length: 15044\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "This is a somewhat shorter issue than usual - being a new parent is a wonderful experience but sometimes the rapidly-developing sentience I care for likes to throw (or in this case, vomit) a curveball at me. Everyone is fine.\n",
            "\n",
            "Synthetic data is being used all across the AI frontier:\n",
            "\n",
            "…It's no longer a question o...\n",
            "Successfully processed: Import AI 369: Conscious machines are possible; AI agents; the varied uses of synthetic data\n",
            "Processing article: Import AI 368: 500% faster local LLMs; 38X more efficient red teaming; AI21's Frankenmodel\n",
            "Attempting to summarize text of length: 22701\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Microsoft researchers figure out how to squeeze more efficiency out of NVIDIA GPUs running LLMs:\n",
            "\n",
            "…The datacenter isn't just a computer, the datacenter is THE BRAIN…\n",
            "\n",
            "Researchers with University of Illinois at Urbana-Champaign and Microsoft Azure Research have studied energy efficiency and performance tradeoffs ...\n",
            "Successfully processed: Import AI 368: 500% faster local LLMs; 38X more efficient red teaming; AI21's Frankenmodel\n",
            "Processing article: Import AI 367: Google's world-spanning model; breaking AI policy with evolution; $250k for alignment benchmarks\n",
            "Attempting to summarize text of length: 15160\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Google plans a world-spanning AI system - and the path to it is through breaking AI policy:\n",
            "\n",
            "…Distributed PAth COmposition (DiPaCo) is a clever idea with big implications…\n",
            "\n",
            "Google has published DIstributed PAth COmposition (DiPaCo), a technique for scaling up the size of neural nets across geographically distrib...\n",
            "Successfully processed: Import AI 367: Google's world-spanning model; breaking AI policy with evolution; $250k for alignment benchmarks\n",
            "Processing article: Import AI 366: 500bn text tokens; Facebook vs Princeton; why small government types hate the Biden EO\n",
            "Attempting to summarize text of length: 22803\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "DROID - another huge robot dataset drops:\n",
            "\n",
            "…More and more data means more and more invention…\n",
            "\n",
            "A consortium of researchers have released the Distributed Robot Interaction Dataset (DROID), a giant dataset of an industrial robot carrying out various tasks in various settings. Datasets like DROID are meant to help ...\n",
            "Successfully processed: Import AI 366: 500bn text tokens; Facebook vs Princeton; why small government types hate the Biden EO\n",
            "Processing article: Import AI 365: WMD benchmark; Amazon sees $1bn training runs; DeepMind gets closer to its game-playing dream\n",
            "Attempting to summarize text of length: 18118\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Anti-doomer DC nonprofit launches:\n",
            "\n",
            "…The counterreaction to overreach on safety…\n",
            "\n",
            "Some technologists have launched Alliance for the Future (AFTF), a DC-based nonprofit organization meant to fight AI safety forces linked to regulatory capture and perceived overreach. “AFTF works to inform the media, lawmakers, an...\n",
            "Successfully processed: Import AI 365: WMD benchmark; Amazon sees $1bn training runs; DeepMind gets closer to its game-playing dream\n",
            "Processing article: Import AI 364: Robot scaling laws; human-level LLM forecasting; and Claude 3\n",
            "Attempting to summarize text of length: 25959\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Scaling laws are coming for real world robots as well:\n",
            "\n",
            "…Which means robots are about to get really, really good, really, really quickly…\n",
            "\n",
            "UC Berkeley researchers have trained a robotic control system that can easily transfer to the real world and have used it to help a bipedal robot from Agility Robotics walk a...\n",
            "Successfully processed: Import AI 364: Robot scaling laws; human-level LLM forecasting; and Claude 3\n",
            "Processing article: Import AI 363: ByteDance's 10k GPU training run; PPO vs REINFORCE; and generative everything\n",
            "Attempting to summarize text of length: 13277\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Turn still photos into video games with Genie:\n",
            "\n",
            "…DeepMind figures out how to turn anything in reality into a controllable game…\n",
            "\n",
            "Google DeepMind has built Genie, a generative model that can create interactive worlds. Genie is a very interesting system, fusing ideas from large-scale generative models with DeepMin...\n",
            "Successfully processed: Import AI 363: ByteDance's 10k GPU training run; PPO vs REINFORCE; and generative everything\n",
            "Processing article: Import AI 362: Amazon's big speech model; fractal hyperparameters; and Google's open models\n",
            "Attempting to summarize text of length: 10803\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Amazon trains a big text-to-speech model via its ‘Amazon AGI’ research team:\n",
            "\n",
            "…No surprises here: Scaling laws work for TTS systems as well…\n",
            "\n",
            "Amazon has built a large text-to-speech model family on 100k hours of public domain speech data. The model, Big Adaptive Streamable TTS with Emergent abilities (BASE), com...\n",
            "Successfully processed: Import AI 362: Amazon's big speech model; fractal hyperparameters; and Google's open models\n",
            "Processing article: Import AI 361: GPT-4 hacking; theory of minds in LLMs; and scaling MoEs + RL\n",
            "Attempting to summarize text of length: 19726\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "DeepMind figures out how to use MoEs to scale-up RL:\n",
            "\n",
            "…Maybe scaling laws are coming for RL as well…\n",
            "\n",
            "Researchers with Google DeepMind, Mila, Universite de Montreal, University of Oxford, and McGill University have figured out how to integrate Mixture-of-Expert models with RL agents. This might make RL agents (e...\n",
            "Successfully processed: Import AI 361: GPT-4 hacking; theory of minds in LLMs; and scaling MoEs + RL\n",
            "Processing article: Import AI 360: Guessing emotions; drone targeting dataset; frameworks for AI alignment\n",
            "Attempting to summarize text of length: 16737\n",
            "Error in summarize_text: index out of range in self\n",
            "Problematic text: Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this (and comment on posts!) please subscribe.\n",
            "\n",
            "Teaching machines to guess at our own emotions with FindingEmo:\n",
            "\n",
            "…~25,000 images to help machines figure out how we’re feeling about stuff…\n",
            "\n",
            "Researchers with KU Leuven have built and released FindingEmo, a dataset for teaching AI systems to classify the emotions of people in complicated photos. FindingEmo consis...\n",
            "Successfully processed: Import AI 360: Guessing emotions; drone targeting dataset; frameworks for AI alignment\n",
            "Summaries written to ai_news_summaries_20240718_081432.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e5063cc6e30b>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mprocess_feeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run once immediately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mrun_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Then start the scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-e5063cc6e30b>\u001b[0m in \u001b[0;36mrun_scheduler\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import feedparser\n",
        "from newspaper import Article\n",
        "from transformers import pipeline\n",
        "from datetime import datetime\n",
        "import schedule\n",
        "import time\n",
        "import os\n",
        "\n",
        "# List of AI news RSS feeds\n",
        "AI_RSS_FEEDS = [\n",
        "    \"https://www.theguardian.com/uk/technology/artificialintelligence/rss\",\n",
        "    \"https://importai.substack.com/feed\"\n",
        "]\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def fetch_and_parse_feed(feed_url):\n",
        "    \"\"\"Fetch and parse an RSS feed.\"\"\"\n",
        "    return feedparser.parse(feed_url)\n",
        "\n",
        "def extract_article_content(url):\n",
        "    \"\"\"Extract the main content of an article.\"\"\"\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return article.text\n",
        "\n",
        "def summarize_text(text, max_length=150):\n",
        "    \"\"\"Summarize the given text.\"\"\"\n",
        "    try:\n",
        "        if not text or len(text.split()) < 30:\n",
        "            print(f\"Text too short to summarize: {text[:100]}...\")\n",
        "            return text\n",
        "\n",
        "        print(f\"Attempting to summarize text of length: {len(text)}\")\n",
        "        summary = summarizer(text, max_length=max_length, min_length=50, do_sample=False)\n",
        "\n",
        "        if summary and len(summary) > 0:\n",
        "            return summary[0]['summary_text']\n",
        "        else:\n",
        "            print(\"Summary generation produced empty result\")\n",
        "            return \"Summary generation failed.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarize_text: {str(e)}\")\n",
        "        print(f\"Problematic text: {text[:500]}...\")  # Print the first 500 characters of the text\n",
        "        return \"Error in summarization.\"\n",
        "\n",
        "def write_summaries_to_file(summaries):\n",
        "    \"\"\"Write summaries to a local file.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"ai_news_summaries_{timestamp}.txt\"\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"AI News Summaries - {datetime.now().strftime('%Y-%m-%d')}\\n\\n\")\n",
        "            f.write(\"\\n\".join(summaries))\n",
        "        print(f\"Summaries written to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to file: {str(e)}\")\n",
        "\n",
        "def process_feeds():\n",
        "    \"\"\"Process all feeds, generate summaries, and write to file.\"\"\"\n",
        "    summaries = []\n",
        "    for feed_url in AI_RSS_FEEDS:\n",
        "        try:\n",
        "            feed = fetch_and_parse_feed(feed_url)\n",
        "            print(f\"Processing feed: {feed_url}\")\n",
        "            print(f\"Number of entries: {len(feed.entries)}\")\n",
        "\n",
        "            for entry in feed.entries:\n",
        "                if 'link' in entry and 'title' in entry:\n",
        "                    try:\n",
        "                        print(f\"Processing article: {entry.title}\")\n",
        "                        content = extract_article_content(entry.link)\n",
        "                        summary = summarize_text(content)\n",
        "                        summaries.append(f\"Title: {entry.title}\\nLink: {entry.link}\\nPublished: {entry.get('published', 'No date available')}\\nSummary: {summary}\\n\\n\")\n",
        "                        print(f\"Successfully processed: {entry.title}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing article {entry.get('title', 'Unknown')}: {str(e)}\")\n",
        "                else:\n",
        "                    print(f\"Skipping entry due to missing link or title\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing feed {feed_url}: {str(e)}\")\n",
        "\n",
        "    # Write summaries to file\n",
        "    if summaries:\n",
        "        write_summaries_to_file(summaries)\n",
        "    else:\n",
        "        print(\"No summaries generated.\")\n",
        "\n",
        "def run_scheduler():\n",
        "    \"\"\"Run the feed processing job on a schedule.\"\"\"\n",
        "    schedule.every().day.at(\"00:00\").do(process_feeds)\n",
        "\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_feeds()  # Run once immediately\n",
        "    run_scheduler()  # Then start the scheduler"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4g0nLw7FPH2HjgRmbzgSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}